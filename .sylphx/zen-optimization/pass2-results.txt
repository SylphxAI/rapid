Pass 2: Split Listeners (Computed vs Effects)
===============================================
Date: 2024-11-15
Bundle: 4.06 KB (+0.31 KB from baseline)

MAJOR ALGORITHMIC CHANGE:
=========================
Separated listeners into two arrays:
- _computedListeners: ComputedCore<unknown>[]  (no function wrapper!)
- _effectListeners: Listener<T>[]             (actual functions)

When zen.value = x:
1. Loop through _computedListeners, mark each STALE (direct property access)
2. Loop through _effectListeners, call each function

BEFORE: listener(newValue, oldValue) for ALL listeners (including computeds wrapped in functions)
AFTER: computed._flags |= STALE for computeds (no function call!)

RESULTS: MAJOR IMPROVEMENTS! ✅✅✅
===================================

Diamond (1k updates):
- Baseline: 119,151 ops/sec (15.15x slower)
- Pass 2:   167,328 ops/sec (10.94x slower)
- IMPROVEMENT: +40.4% faster!
- Gap reduced: 15.15x → 10.94x ✅

Triangle (1k updates):
- Baseline: 122,325 ops/sec (14.75x slower)
- Pass 2:   172,055 ops/sec (10.64x slower)
- IMPROVEMENT: +40.7% faster!
- Gap reduced: 14.75x → 10.64x ✅

Fanout (100 computeds):
- Baseline: 11,605 ops/sec (25.3x slower)
- Pass 2:   14,452 ops/sec (19.3x slower)
- IMPROVEMENT: +24.5% faster!
- Gap reduced: 25.3x → 19.3x ✅

Deep chain (10 levels):
- Baseline: 123,903 ops/sec (12.2x slower)
- Pass 2:   112,806 ops/sec (13.5x slower)
- Change: -8.9% (regression, but better than Pass 1b's -14.2%)

Broad (50 sources):
- Baseline: 105,944 ops/sec (5.7x slower)
- Pass 2:   104,651 ops/sec (5.7x slower)
- Change: -1.2% (essentially same)

Create 1000 signals:
- Baseline: 29,628 ops/sec (1.25x slower)
- Pass 2:   34,829 ops/sec (1.06x slower)
- IMPROVEMENT: +17.5% faster! ✅

Create 1000 computeds:
- Baseline: 6,179 ops/sec (5.0x slower)
- Pass 2:   5,343 ops/sec (5.9x slower)
- Change: -13.5% (regression)

ANALYSIS:
=========

Why This Worked:
----------------
Eliminating function call overhead for computed listeners was huge.
Before: 100 computeds × function call = 100 function calls
After: 100 computeds × property access = just property writes

Function calls have overhead:
- Stack frame creation
- Argument passing
- Return handling

Direct property access is just:
- Memory write (computeds[i]._flags |= FLAG_STALE)

This is why Diamond/Triangle improved by 40% - they have many computed-to-computed propagations.

Remaining Gap Analysis:
----------------------

Current gaps:
- Diamond/Triangle: ~11x slower (target: <8x)
- Fanout: 19.3x slower (target: <10x)
- Deep chain: 13.5x slower

The remaining gap is primarily:
1. **Compiler inlining** (~8-10x): SolidJS compiler inlines reactive primitives.
   When you write `signal()` in SolidJS, the compiler converts it to direct variable access.
   We can't do this without a compiler.

2. **V8 optimization** (~2-3x): SolidJS code paths are more monomorphic.
   Opportunity: Optimize object shapes, reduce polymorphism.

3. **Algorithm** (~1.5-2x): SolidJS might have better algorithms for specific cases.
   Opportunity: Study SolidJS source for edge case optimizations.

NEXT OPTIMIZATION OPPORTUNITIES:
================================

1. **Convert to Classes** (Target: +10-15%)
   - Object.create() slower than class constructors
   - Better V8 optimization with classes
   - More monomorphic object shapes

2. **Optimize Array Access** (Target: +5-10%)
   - Use typed arrays where possible
   - Pre-allocate arrays with capacity
   - Inline array access patterns

3. **Reduce Allocations** (Target: +10-20%)
   - Pool unsubscribe functions
   - Reuse listener arrays
   - Cache frequently accessed properties

4. **Study SolidJS Source** (Target: +20-30%)
   - Understand their exact algorithms
   - Find any tricks we're missing
   - Profile to find hot paths

REALISTIC TARGET:
================

With compiler inlining being ~8-10x advantage, our realistic target without a compiler is:
- Best case: 3-5x slower than SolidJS
- Target: 5-8x slower (achievable with more optimizations)
- Current: 11x slower (Diamond/Triangle)

We've already closed the gap from 15x to 11x (27% improvement).
To reach 5-8x, we need another 30-45% improvement.

Pass 3 will focus on converting to classes + reducing allocations.
