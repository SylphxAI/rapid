Pass 1b: Strong Typing (indexOf restored)
===========================================
Date: 2024-11-15
Bundle: 3.85 KB (+0.10 KB from baseline)

Changes from Pass 1:
- Restored indexOf (V8 optimizes it better than manual loops)

RESULTS: Mixed (minor improvements + regressions)
==================================================

Diamond (1k updates):
- Baseline: 119,151 ops/sec
- Pass 1b:  117,521 ops/sec
- Change: -1.4% (minor regression)

Triangle (1k updates):
- Baseline: 122,325 ops/sec
- Pass 1b:  118,935 ops/sec
- Change: -2.8% (minor regression)

Fanout (100 computeds):
- Baseline: 11,605 ops/sec
- Pass 1b:  11,588 ops/sec
- Change: -0.1% (essentially same)

Deep chain (10 levels):
- Baseline: 123,903 ops/sec
- Pass 1b:  106,330 ops/sec
- Change: -14.2% REGRESSION ⚠️

Broad (50 sources):
- Baseline: 105,944 ops/sec
- Pass 1b:  110,663 ops/sec
- Change: +4.5% IMPROVEMENT ✅

Create 1000 signals:
- Baseline: 29,628 ops/sec
- Pass 1b:  35,459 ops/sec
- Change: +19.7% IMPROVEMENT ✅

Create 1000 computeds:
- Baseline: 6,179 ops/sec
- Pass 1b:  5,625 ops/sec
- Change: -9.0% regression

ANALYSIS:
=========

Positives:
- Signal creation: +19.7% (strong typing helps object creation)
- Broad: +4.5% improvement

Negatives:
- Deep chain: -14.2% regression (concerning)
- Most others: -1% to -3% (minor)

Conclusion: Strong typing alone doesn't solve the performance gap.
The core issue remains: we're doing double loops for listener notification.

NEXT STRATEGY:
==============

Stop focusing on micro-optimizations (typing, indexOf, etc).
Focus on ALGORITHMIC changes:

1. Separate computed listeners from effect listeners
   - Computed: only mark STALE (no function call)
   - Effect: mark STALE + call listener
   - This cuts fanout overhead in half

2. This requires changing listener structure from:
   Array<Listener> to { computeds: ComputedCore[], effects: Listener[] }

3. Expected improvement: 30-50% on fanout scenarios
